
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# Delta Lake Configuration  
# 없으면 Delta 테이블 읽기/쓰기 불가능
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Spark가 테이블 정보를 관리하는 방식을 내장(in-memory) 방식이 아닌, Hive 방식으로 강제한다.
# Hive Metastore - 테이블 메타데이터 저장소
spark.sql.catalogImplementation=hive
# Spark가 내장 Metastore 대신, Standalone Metastore 서비스를 바라보도록 설정
spark.sql.hive.metastore.uris=thrift://hive-metastore:9083

# Metastore Warehouse 경로를 지정한다.
hive.metastore.warehouse.dir=s3a://warehouse/

spark.driver.extraClassPath=/opt/spark/jars/*