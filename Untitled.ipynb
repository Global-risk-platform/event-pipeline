{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9ff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 16:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! SparkSession 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. 최종 SparkSession 생성 (JAR 직접 사용)\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter_Final_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\",\"/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "print(\"!! SparkSession 생성 완료!\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ef22c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 16:20:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDELT silver 데이터 로드 성공: s3a://silver/gdelt_events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 16:21:10 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 레코드 수 : 40\n",
      "컬럼 수: 9\n",
      "\n",
      " 테이블 스키마: \n",
      "root\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- Actor1CountryCode: string (nullable = true)\n",
      " |-- EventRootCode: string (nullable = true)\n",
      " |-- day_of_week: long (nullable = true)\n",
      " |-- event_count: long (nullable = true)\n",
      " |-- avg_conflict_score: double (nullable = true)\n",
      " |-- stddev_conflict_score: double (nullable = true)\n",
      " |-- avg_tone: double (nullable = true)\n",
      " |-- unique_source_count: long (nullable = true)\n",
      "\n",
      "\n",
      " 상위 20개 레코드\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-------------+-----------+-----------+------------------+---------------------+--------+-------------------+\n",
      "|event_date|Actor1CountryCode|EventRootCode|day_of_week|event_count|avg_conflict_score|stddev_conflict_score|avg_tone|unique_source_count|\n",
      "+----------+-----------------+-------------+-----------+-----------+------------------+---------------------+--------+-------------------+\n",
      "|null      |CZE              |null         |null       |83403      |null              |null                 |null    |null               |\n",
      "|null      |MRT              |null         |null       |10284      |null              |null                 |null    |null               |\n",
      "|null      |ZAF              |null         |null       |118052     |null              |null                 |null    |null               |\n",
      "|null      |NPL              |null         |null       |76284      |null              |null                 |null    |null               |\n",
      "|null      |HTI              |null         |null       |68538      |null              |null                 |null    |null               |\n",
      "|null      |ERI              |null         |null       |6970       |null              |null                 |null    |null               |\n",
      "|null      |AFR              |null         |null       |378985     |null              |null                 |null    |null               |\n",
      "|null      |SVK              |null         |null       |62191      |null              |null                 |null    |null               |\n",
      "|null      |BHR              |null         |null       |29881      |null              |null                 |null    |null               |\n",
      "|null      |EAF              |null         |null       |809        |null              |null                 |null    |null               |\n",
      "|null      |ECU              |null         |null       |52355      |null              |null                 |null    |null               |\n",
      "|null      |LIE              |null         |null       |2113       |null              |null                 |null    |null               |\n",
      "|null      |VNM              |null         |null       |144289     |null              |null                 |null    |null               |\n",
      "|null      |CHL              |null         |null       |46939      |null              |null                 |null    |null               |\n",
      "|null      |CUB              |null         |null       |84001      |null              |null                 |null    |null               |\n",
      "|null      |EST              |null         |null       |42325      |null              |null                 |null    |null               |\n",
      "|null      |SEA              |null         |null       |36421      |null              |null                 |null    |null               |\n",
      "|null      |LBY              |null         |null       |38504      |null              |null                 |null    |null               |\n",
      "|null      |IDN              |null         |null       |298152     |null              |null                 |null    |null               |\n",
      "|null      |HUN              |null         |null       |164819     |null              |null                 |null    |null               |\n",
      "+----------+-----------------+-------------+-----------+-----------+------------------+---------------------+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 16:22:27 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-17bffd0e-e88c-4817-b133-10a96e31c72e/userFiles-e9de11e7-c80d-463c-96b3-e13f78716510. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-17bffd0e-e88c-4817-b133-10a96e31c72e/userFiles-e9de11e7-c80d-463c-96b3-e13f78716510\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:170)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:113)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:94)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1231)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1509)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2175)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2081)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$31(SparkContext.scala:664)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.io.IOException: Cannot run program \"rm\": error=0, Failed to exec spawn helper: pid: 225, signal: 15\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:166)\n",
      "\t... 23 more\n",
      "Caused by: java.io.IOException: error=0, Failed to exec spawn helper: pid: 225, signal: 15\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:314)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:244)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\n",
      "\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "silver_path = \"s3a://silver/gdelt_events\"\n",
    "try:\n",
    "    df_gdelt_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "    print(f\"GDELT silver 데이터 로드 성공: {silver_path}\")\n",
    "\n",
    "    # 기본 정보 출력\n",
    "    print(f\"총 레코드 수 : {df_gdelt_silver.count()}\")\n",
    "    print(f\"컬럼 수: {len(df_gdelt_silver.columns)}\")\n",
    "\n",
    "    # 스키마 정보\n",
    "    print(\"\\n 테이블 스키마: \")\n",
    "    df_gdelt_silver.printSchema()\n",
    "\n",
    "    # 상위 20개 \n",
    "    print(\"\\n 상위 20개 레코드\")\n",
    "    df_gdelt_silver.show(20, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"데이터 로드 실패: {e}\")\n",
    "    print(\"1. MinIo에 Silver 버킷이 존재하는지\")\n",
    "    print(\"2. gdelt_events 폴더에 Delta 파일이 있는지\")\n",
    "    print(\"3. 데이터 파이프라인이 실행되었는지\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433bd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 15:01:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 클러스터 모드 SparkSession 생성 완료!\n",
      "Master URL: spark://spark-master:7077\n",
      "Application ID: app-20250818150131-0000\n",
      "Default Parallelism: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 크기: 1000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|count|\n",
      "+--------+-----+\n",
      "|       0|  100|\n",
      "|       1|  100|\n",
      "|       2|  100|\n",
      "|       3|  100|\n",
      "|       4|  100|\n",
      "|       5|  100|\n",
      "|       6|  100|\n",
      "|       7|  100|\n",
      "|       8|  100|\n",
      "|       9|  100|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 15:02:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/08/18 15:02:37 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클러스터 모드에서 Delta Lake 저장 성공: s3a://bronze/cluster-test\n"
     ]
    }
   ],
   "source": [
    "# Spark 클러스터 모드 테스트\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "spark_cluster = SparkSession.builder \\\n",
    "    .appName(\"Cluster_Mode_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\",\"/opt/spark/jars/delta-core_2.12-2.4.0.jar,/opt/spark/jars/delta-storage-2.4.0.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/aws-java-sdk-bundle-1.12.367.jar\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark 클러스터 모드 SparkSession 생성 완료!\")\n",
    "print(f\"Master URL: {spark_cluster.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark_cluster.sparkContext.applicationId}\")\n",
    "print(f\"Default Parallelism: {spark_cluster.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# 클러스터 테스트 데이터 생성\n",
    "import pandas as pd\n",
    "data = {'id': range(1000), 'value': [f'cluster_value_{i}' for i in range(1000)], 'category': [i % 10 for i in range(1000)]}\n",
    "pdf = pd.DataFrame(data)\n",
    "df_cluster_test = spark_cluster.createDataFrame(pdf)\n",
    "print(f\"테스트 데이터 크기: {df_cluster_test.count()} rows\")\n",
    "\n",
    "# 클러스터에서 집계 작업 테스트\n",
    "result = df_cluster_test.groupBy(\"category\").count().orderBy(\"category\")\n",
    "result.show()\n",
    "\n",
    "# Delta Lake 저장 테스트\n",
    "cluster_path = \"s3a://bronze/cluster-test\"\n",
    "df_cluster_test.write.format(\"delta\").mode(\"overwrite\").save(cluster_path)\n",
    "print(f\"클러스터 모드에서 Delta Lake 저장 성공: {cluster_path}\")\n",
    "spark_cluster.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
